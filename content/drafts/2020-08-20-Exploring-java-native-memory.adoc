---
authors: ["brice.dutheil"]
date: "2020-05-23T23:45:29+02:00"
language: en
#tags: ["cgroup", "java", "kubernetes", "docker", "memory", "jcmd", "heap", "procfs", "pmap"]
slug: "exploring-jvm-native-memory"
title: "Exploring JVM native memory"
draft: true
---

While `ps` is a good indicator, it is sometime useful to understand a bit more
how much JVM components are actually consuming memory.

// hugo manual page summary divider
++++
<!--more-->
++++

// TODO: Talk about the kubernetes scheduling aspect of setting memory request and limits ?

// TODO: Change JDK links to openjdk/jdk

// Related articles
// - https://dev.to/wayofthepie/jvm-basic-memory-overview-535m
// - http://trustmeiamadeveloper.com/2016/03/18/where-is-my-memory-java/

_This entry has been marinating for mast of the year 2020. Rewriting it 3 times
to make it more digestible form, then I found out I should just split my thoughts
in articles a bit smaller that hopefully make sense on their own._

I'd like to thank https://twitter.com/pingtimeout[Pierre Laporte],
https://twitter.com/olivierbourgain[Olivier Bourgain], https://twitter.com/blemale[Bastien Lemale],
and https://twitter.com/ylegat[Yohan Legat] for the review.


'''

This entry is about figuring out the native memory _parts_ of a Java process.
All snippet comes from Kubernetes pods under load, in production. The actual
problem could be solved by adjusting cgroup limits, but the need to understand and
other factors served as an excuse to go down the rabbit hole.

[TIP]
====
*tl;dr*

* Set the cgroup memory limit high enough, so the application isn't oomkilled, this will
let you analyze how the app work, and adjust settings without fear (except from your
colleagues that compare everything to _Go_ or _rust_).

* If you don't think it's Java heap memory leak, i.e. the heap usage isn't alarming,
try to inspect native memory, it's easier with the flag `-XX:+AlwaysPreTouch`,
however keep in mind this will bump your RSS right from the start, so anticipate this
increase in your cgroup memory limit.

* Then inspect what's outside the JVM Java heap.

====

In doing so the following writing tries to piece together elements
from a few things I knew, things I grepped in the JDK codebase, blog posts, stack overflow,
and things learned from -- awesome -- people.

If I ignored something or if I'm wrong please reach out.

[NOTE]
====
Most of the time, figures will use the https://en.wikipedia.org/wiki/Binary_prefix[IEC binary notation] (`1 KiB = 1024 B`),
it matches the https://github.com/corretto/corretto-11/blob/055a9a1a279b9a2953c2150bc937b04f905eeba1/src/src/hotspot/share/utilities/globalDefinitions.hpp#L226[JVM],
our https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#meaning-of-memory[Kubernetes] usage,
and Linux's tools (`/proc/{pid}/stat` or `/proc/{pid}/maps` ; although I couldn't find a reference link stating this).

Some charts may however use the https://en.wikipedia.org/wiki/Binary_prefix[SI metric notation] (`1 KB = 1000 B`).

[quote, Java Performance: The Definitive Guide, Getting the Most Out of Your Code (1st Edition)]
Actually, 227,893 KB is only 222 MB. For ease of discussion, I'll truncate the KBs part by 1,000
in this chapter; pretend I'm a disk manufacturer.

_Thanks to this https://twitter.com/fleming_matt/status/1282729134481965064?s=21[tweet]._
====


The motivation behind this exploration is to help understand: What is consuming the memory ?






== Exploration begins

=== The JVM flags as a starting point

When assessing java memory, one of the fist thing to look at are the Java heap parameters.

It's likely anyone that reads this article is familiar with `Xms` or `Xmx`, but there are
other ways to define the boundaries of the Java heap in particular if the process is started
with `\*RAMPercentage`. With these the JVM will compute the actual values from the `cgroup`,
in this case it's possible to access the actual runtime values with `jcmd`.

In short, it's possible to look at the command line options, but using the diagnostic
command `jcmd {pid} VM.flags` lets you peek at the actual values that the JVM used.

For example with a memory limit of `5 GiB`, if a process is started with
`-XX:InitialRAMPercentage=85.0 -XX:MaxRAMPercentage=85.0` the `VM.flags`
diagnostic command will output this :

.JVM current flags in a kubernetes container
[source, bash]
----
$ jcmd $(pidof java) VM.flags | tr ' ' '\n'
6:
...
-XX:InitialHeapSize=4563402752 <3>
-XX:InitialRAMPercentage=85.000000 <1>
-XX:MarkStackSize=4194304
-XX:MaxHeapSize=4563402752 <4>
-XX:MaxNewSize=2736783360
-XX:MaxRAMPercentage=85.000000 <2>
-XX:MinHeapDeltaBytes=2097152
-XX:NativeMemoryTracking=summary
...
----
<1> Initial RAM at 85%
<2> Max RAM at 85%
<3> Initial heap size ~`4.25 GiB`
<4> Max heap size ~`4.25 GiB`

TIP: Do not confuse the `VM.flags` command which will output parameters calculated *from* the
_command line_ and `VM.command_line` which will print the *raw* _command line_.

The other Hotspot flag values comes are JVM _defaults_, which may either be static values,
or computed from internal heuristics.

As we tend to dismiss regularly, the Java heap is only a part of the process memory usage.
So now let's dig into how memory is _consumed_. The values or snippet comes from an
application running inside a container.





=== The real memory footprint of the java process in the container

In the Java dream us developers shouldn't care much about the memory or the OS
and if we ever had to we should only look at the Java Heap usage.

This dream more or less lasted a long time until Java applications were put
in containers.

Indeed, one of the most critical things to look at, especially in a containers,
is the _resident set size_, it can be obtained in various ways, using `ps`, `top` or
reading the `/proc` filesystem. E.g. on the same application as above

.`ps`
[source, role="primary"]
----
$ ps o pid,rss -p $(pidof java)
PID   RSS
  6 4701120
----

.`/proc/{pid}/status`
[source, role="secondary"]
----
$ cat /proc/$(pgrep java)/status | grep VmRSS
VmRSS:	 4701120 kB
----

The RSS is `4.6 GiB`, and it's Java heap size is `4.25 GiB`, indicating
this process uses around `0.35 GiB` of non-Java heap memory, I'll refer
to this memory as _native memory_.

Now I'd like to dig a bit to understand the reported number `4701120 KiB`,
what it actually measures.






==== The JVM component memory

In order to understand how the Java process memory is consumed, we need to use
_Native Memory Tracking_ (`-XX:NativeMemoryTracking=summary`) which produces
an overview of the memory usage by the _components of the JVM_. It actually gives
a pretty good picture of the "cost" of having a JVM.

NOTE: Enabling _detailed_ native memory tracking (NMT) causes a 5% to 10%
performance overhead. The _summary_ mode merely has an impact in memory usage
as shown below and is usually enough.

NOTE: It is necessary to note that while the above command indicate a scale
in `KB` for the JVM it really means `KiB`.


.JVM native memory trcking report
[source, bash]
----
$ jcmd $(pidof java) VM.native_memory
6:

Native Memory Tracking:

Total: reserved=7168324KB, committed=5380868KB                               <1>
-                 Java Heap (reserved=4456448KB, committed=4456448KB)        <2>
                            (mmap: reserved=4456448KB, committed=4456448KB)

-                     Class (reserved=1195628KB, committed=165788KB)         <3>
                            (classes #28431)                                 <4>
                            (  instance classes #26792, array classes #1639)
                            (malloc=5740KB #87822)
                            (mmap: reserved=1189888KB, committed=160048KB)
                            (  Metadata:   )
                            (    reserved=141312KB, committed=139876KB)
                            (    used=135945KB)
                            (    free=3931KB)
                            (    waste=0KB =0.00%)
                            (  Class space:)
                            (    reserved=1048576KB, committed=20172KB)
                            (    used=17864KB)
                            (    free=2308KB)
                            (    waste=0KB =0.00%)

-                    Thread (reserved=696395KB, committed=85455KB)
                            (thread #674)
                            (stack: reserved=692812KB, committed=81872KB)    <5>
                            (malloc=2432KB #4046)
                            (arena=1150KB #1347)

-                      Code (reserved=251877KB, committed=105201KB)          <6>
                            (malloc=4189KB #11718)
                            (mmap: reserved=247688KB, committed=101012KB)

-                        GC (reserved=230739KB, committed=230739KB)          <7>
                            (malloc=32031KB #63631)
                            (mmap: reserved=198708KB, committed=198708KB)

-                  Compiler (reserved=5914KB, committed=5914KB)              <8>
                            (malloc=6143KB #3281)
                            (arena=180KB #5)

-                  Internal (reserved=24460KB, committed=24460KB)           <10>
                            (malloc=24460KB #13140)

-                     Other (reserved=267034KB, committed=267034KB)         <11>
                            (malloc=267034KB #631)

-                    Symbol (reserved=28915KB, committed=28915KB)            <9>
                            (malloc=25423KB #330973)
                            (arena=3492KB #1)

-    Native Memory Tracking (reserved=8433KB, committed=8433KB)
                            (malloc=117KB #1498)
                            (tracking overhead=8316KB)

-               Arena Chunk (reserved=217KB, committed=217KB)
                            (malloc=217KB)

-                   Logging (reserved=7KB, committed=7KB)
                            (malloc=7KB #266)

-                 Arguments (reserved=19KB, committed=19KB)
                            (malloc=19KB #521)

-                    Module (reserved=1362KB, committed=1362KB)
                            (malloc=1362KB #6320)

-              Synchronizer (reserved=837KB, committed=837KB)
                            (malloc=837KB #6877)

-                 Safepoint (reserved=8KB, committed=8KB)
                            (mmap: reserved=8KB, committed=8KB)

-                   Unknown (reserved=32KB, committed=32KB)
                            (mmap: reserved=32KB, committed=32KB)
----
<1> This shows a `reserved` value (`7168324 KiB` (~`6.84 GiB`)), it's the amount
of addressable memory on that container, and a `committed` value (`4456448 KiB` (~`4.25 GiB`))
that represents what the JVM actually asked the OS to allocate.
<2> `Heap` zone, note that reserved and committed values are the same `4456448 KiB`
here because our `InitialRAMPercentage` is the same as max. I'm not sure why this number
is different from the VM flags `-XX:MaxHeapSize=4563402752` though.
<3> ~`162 MiB` of metaspace.
<4> How many classes have been loaded : `28431`.
<5> There are 674 threads whose stacks are using ~`80 MiB` at this time.
<6> `Code` cache area (assembly of the used methods) ~`102 MiB` out of ~`246 MiB`.
<7> This section contains `GC` algorithms internal data structures, this is app
is using G1GC which takes ~`225 MiB`.
<8> C1 / C2 compilers (which compile bytecode to assembly) use ~`5.8 MiB`.
<9> The `Symbol` section contains many things like interned strings and other
internal constants for about `28.2 MiB`.
<10> The `Internal` area takes ~`24 MiB`. Before Java 11 this area included
`DirectByteBuffers`, but from Java 11 those are accounted in the `Other` zone.
<11> The `Other` section after Java 11 includes `DirectByteBuffers` ~`261 MiB`.

The remaining areas are much smaller in scale, NMT takes ~`8.2 MiB` itself, module system usage ~`1.3 MiB`,
etc. Also, note that enabling other JVM features may show up if they are activated, like flight recorder.
https://docs.oracle.com/en/java/javase/11/troubleshoot/diagnostic-tools.html#GUID-5EF7BB07-C903-4EBD-A9C2-EC0E44048D37[Source]

There's a lot more to read on the
https://docs.oracle.com/en/java/javase/11/vm/native-memory-tracking.html#GUID-39676837-DA61-4F8D-9C5B-9DB1F5147D80[official documentation about NMT]
and https://docs.oracle.com/en/java/javase/11/troubleshoot/diagnostic-tools.html#GUID-1F53A50E-86FF-491D-A023-8EC4F1D1AC77[how to Monitor VM Internal Memory].
Yet another worthwhile read on https://shipilev.net/jvm/anatomy-quarks/12-native-memory-tracking/[native memory tracking]
by http://twitter.com/shipilev[Aleksey ShipilÑ‘v].

*In the rest of this article whe n talking the context of Native Memory Tracking
I may use the term _memory type_ or _memory zones_, but the real definition would be :*

> *the _memory allocation type_ performed by a _JVM component_*

The different sections are defined there in
https://github.com/corretto/corretto-11/blob/caa2f4cad666b508a88b92db01054ace8647a820/src/src/hotspot/share/memory/allocation.hpp#L114-L141[this `MemoryType` enumeration],
and https://github.com/corretto/corretto-11/blob/2b351313740f148597cf680d8443df93931de813/src/src/hotspot/share/services/nmtCommon.cpp#L28-L51[here]
as they appear in the report.

_NMT_ is a great tool to gain an insight on the memory usage of the various
parts that compose the Java runtime. It has interesting subcommands to compare
the memory usage of the JVM component with a _baseline_.

However, and that's the important bit *it does not answer
what is actually accounted in the RSS column of `ps`*.







==== Revising OS virtual memory and memory management

I mentioned this acronym already, _RSS_ or **R**esident **S**et **S**ize, what is it?
What exactly means _committed_ memory or _reserved_ memory shown in _NMT_ ? How do they
relate to each other?

First let's break down the vocabulary when we talk about memory.

.memory vocabulary
[ditaa,"memory-vocabulary"]
----

|<--virtual memory----------------------------------------->|
|<--reserved memory--------------------------->|            |
|<--committed memory-------------->|           |            |
:                                  :           :            :
+-------------------+------+-------+-----------+------------+
| addressable space of the process                          |
+-------------------+------+-------+-----------+------------+
|                                                           |
|<--contiguous addresses----------------------------------->|
|                                                           |
0                                                   0x8000000

----


.vocabulary breakdown (https://stackoverflow.com/a/31178912/48136[source])
[%autowidth.stretch]
|===

| *Committed* | Address ranges that have been mapped or ``malloc``ed.
They may or may not be backed by physical or swap due to lazy allocation and paging.
This applies to the JVM and the OS. These ranges are actually not necessarily contiguous.

| *Reserved* | The total address range that has been pre-mapped via `mmap` or `malloc`
for a particular memory pool. In other words _reserved memory_ represents the maximum
addressable memory.
Those could be referred to as *uncommitted*.

| *Resident* | OS memory pages which are currently in physical ram. This means codes,
stacks, part of the committed memory pools but also portions of ``mmap``ed files
which have recently been accessed and allocations outside the control of the JVM.

| *Virtual* | The sum of all virtual address mappings. Covers committed, reserved
memory pools but also mapped files or shared memory. This number is rarely informative
since the JVM will reserve large address ranges upfront. We can see this number
as the pessimistic memory usage.

|===


The graph above does not yet show _resident memory_. Indeed, the above graph most
display the relative size by memory _kind_ within an address space of a process.
In order explain resident memory it's necessary to revise how Linux (and other OSes
by the way) manage memory using the concept of *paging*.

The virtual address space is divided into smaller chunks called _pages_
usually `4 KiB` in size.
_Other page sizes do exist and may even co-exist (e.g. having pages of
4 KiB mixed with 2 MiB pages), it depends on the capabilities of the processor ;
working with different size of pages is something that is out of scope for this article.
What is interesting is how paging and RSS relate to each other._


.virtual memmory and paging (for a single process)
[ditaa,"memory-paging"]
----

+-+ touched/used  +-+ untouched/unused
| | page          : | page
+-+               +-+

|<--virtual memory----------------------------------------->|
|<--reserved memory------------------------------>|         |
|<--committed memory--------------->|             |         |
:                                   :             :         :
+-+=+=+-+=+-+-+-+=+=+=+=+=+-+=+=+=+=+=+=+=+=+=+=+=+=+-+=+=+-+
| | | | | | | | | | | | | | | | | | | | | | | | | | | | | | |
+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
 | |   |   |           |   |                         |     |
 | \\  \\  \----\   /--/   \-----\             /-----/     |
 |  |   |       |   |            |             |  /--------/
 |  |   |       |   |            |             |  |
 v  v   v       v   v            v             v  v
/--+---+--+----+---+---+--------+-----+-----+-------\
|0 |1  |2 |... |40 |50 |...     |1000 |2000 |...    |
+--+---+--+----+---+---+--------+-----+-----+-------+ MMU
|9 |50 |7 |... |2  |   |...     |6000 |6001 |       |
\--+---+--+----+---+---+--------+-----+-----+-------/
  |             |         |       |            |
  |           /-/         |       |            |
  |      /----|-------------------/            |
  |      |    |           |                    |
+-|------|----|--+        |              +-----|--------+
| v      v    v  |        |              |     |        |
| ++ ++ ++ ++ ++ |        |              |     |        |
| ++ ++ ++ ++ ++ |        |              |     v        |
| ++ ++ ++ ++ ++ |        |              |+-------+     |
| ++ ++ ++ ++ ++ |        \-------------->|swap   |     |
| ++ ++ ++ ++ ++ |                       |+-------+     |
+----------------+ RAM                   +--------------+ Disk

----

The graph above shows the addressable space of a process and its _pages_.
The process can access these pages using the addresses of its virtual space,
however these pages have to be stored physically, usually in RAM, sometime on disk.
When referring to these chunks of memory on hardware, we use the term _frame_.

The real memory address is naturally different from this virtual address space
for the process. There's a specialized component called MMU (Memory Management Unit)
that is part of the processor to perform the translation between virtual addresses
and physical addresses.

The incentive behind virtual memory and paging comes from multi-tasking, it allows
running multiple program concurrently. Each process will have the illusion of a single
big block of memory. In practice, it abstracts away useful tricks like
lazy allocation, swapping, file mapping, defragmentation, caching, etc.

The OS is hard at work performing these tricks while keeping this illusion for all
processes. Since programs run concurrently, **not all memory pages is used at the
same time**.

In practical terms we can observe that:

* A physical memory frame won't be used if the process didn't _touch_ a page, or
we can say this page doesn't exist.

* The kernel may choose to move the real location of the page to use a slower device
to store pages, usually a disk, in a special place called _swap_, if it thinks there
won't be enough physical memory (RAM).

* The kernel may use unemployed physical frames for caching purpose, or other tasks
like defragmentation.

The _resident set size_ mean the total set of pages of a process that resides either
in RAM or in secondary storage, i.e. without untouched/unused pages.
This contrasts with VSZ or virtual size which includes the total address space of
a program, this value is usually way superior to RSS.

_If you want to dive how the whole paging thing works head to
system courses, articles (like https://landley.net/writing/memory-faq.txt[this masterpiece])
where they usually explain in depth how everything interacts._

To put things in context I'd like to explain one last thing to memory management
with the JVM perspective.








===== Reserved and committed memory for NMT

As mentioned above, one of the idea of the *reserved* / *committed* memory is to
provide the illusion of a single *continuous* memory space.

Concretely for the JVM it means that

1. the _committed_ memory is immediately usable,
2. and the _reserved_ memory part means memory _put on hold_ and not usable.

With a better understanding of how memory works let's look again at the output
of the `VM.native_memory` command to make more sense of it:

[source, bash]
----
Total: reserved=7168324KB, committed=5380868KB                               <1>
-                 Java Heap (reserved=4456448KB, committed=4456448KB)        <2>
                            (mmap: reserved=4456448KB, committed=4456448KB)
...
-                     Class (reserved=1195628KB, committed=165788KB)         <3>
...
-                    Thread (reserved=696395KB, committed=85455KB)           <4>
...
-                      Code (reserved=251877KB, committed=105201KB)
...
-                        GC (reserved=230739KB, committed=230739KB)          <5>
...
----
<1> The process addressable memory and what is currently committed.
<2> Here the NMT also show the same abstractions of committed and reserved memory,
on this process these values are the same because the `InitialHeapSize` (`Xms`) and
`MaxHeapSize` (`Xmx`)are the same. If these boundaries were different it is likely
the heap zone would show different values for reserved and committed memory; the
JVM will increase the committed memory if necessary, and can even uncommit some of
this memory if the GC algorithm allows it.
<3> Class, Code spaces works the same way, specifics JVM flags control the reserved
and committed memory.
<4> Java Threads are allocated within the process memory, the JVM flags only control
the size of a thread. I will expand on this later.
<5> Then comes the other memory space of the JVM, like the GC internal structures, who
are using a different memory management, these zones usually have the same reserved/committed
amount.

Or with a picture :

.JVM memory allocations
[ditaa, jvm-memory-allocations]
----

|<--virtual memory----------------------------------------------------->|
|<--reserved memory------------------------------------------------->|  |
|<--committed memory--------------------------------------------->|  |  |
|<--heap max size-------->|<--Class reserved--->|<--others-->|    |  |  |
|<--committed heap--->|   |<--Class commited->| |            |    |  |  |
|<--used heap---->|   |   |                   | |            |    |  |  |
:                 :   :   :                   : :            :    :  :  :
+-----------------+---+---+-------------------+-+------------+----+--+--+
| addressable space of the process                                      |
+-------------------+------+-------+-----------+------------------------+
|                                                                       |
|<--contiguous addresses----------------------------------------------->|
|                                                                       |
0                                                               0x8000000

----

This immediately leads to new vocabulary :

.Java memory vocabulary
[%autowidth.stretch]
|===

| *Used Heap* | The amount of memory occupied by live objects and to a certain
extent object that are unreachable but not yet collected by the GC. This only
relate to the JVM Java heap.

| *Committed heap* | The current limit if the writable memory to write objects to.
It's the current workspace of the GC. Upon process start this value should be equal
to `Xms`, then the GC may expand it up to the Java heap reserved memory, or in Java
terms the heap max size, or `Xmx`.

| *Heap Max Size* | The maximum amount of memory that the Java heap can occupy.
It's the _reserved_ amount in Java Heap section of the NMT output.
If the application requires more memory, this will result in a `OutOfMemoryError`.

|===


So committed stands for writable memory and, reserved stands for total addressable
space of the memory. How does it work concretely?

The JVM starts by https://github.com/corretto/corretto-11/blob/3b31d243a19774bebde63df21cc84e994a89439a/src/src/hotspot/os/linux/os_linux.cpp#L3421-L3444[_reserving_ the memory],
then parts of this "reserve" will be made available by
https://github.com/corretto/corretto-11/blob/3b31d243a19774bebde63df21cc84e994a89439a/src/src/hotspot/os/linux/os_linux.cpp#L3517-L3531[modifying the memory mappings]
using `malloc`, `mmap`, as well as `mprotect` calls in particular (on Linux).






===== `malloc` and `mmap`

The `malloc` and `mmap` C calls ask the OS to allocate memory. It's the job of the OS to
provide the application the necessary memory, or fail if it is not possible.

Also, depending on the mapping in particular for `mmap` the OS can be asked to make a file
accessible as a memory zone, in short it's the kernel that perform IOs, in contrast to perform
IOs with a file descriptor application side.

image:../../static/assets/maxrampercentage/malloc-mmap.svg[align="center", title="Simple overview of malloc and mmap"]

.Differences between https://linux.die.net/man/3/malloc[`malloc`] and http://www.kernel.org/doc/man-pages/online/pages/man2/mmap.2.html[`mmap`]
[%collapsible]
====
* `malloc` may _recycle_ previously used memory that was released by `free`,
and perform a system call to get memory only required. It's part of the C standard.

* `malloc` allows you pass a size and that's basically it.

* `mmap` is a system call. It's not part of the C standard, and may not be available
on all platforms.

* `mmap` can both map private memory or shared memory (as in shared with other processes).
Those are called _anonymous mapping_ using flag `MAP_ANONYMOUS`.

* `mmap` can also interact with disk files on specific ranges, without having
a file descriptor.

* `mmap` can be set with various flags that are used to control how this memory
mapping behave.

* Both have their performance characteristics, `malloc` is usually preferred for
few and small allocations, `mmap` is preferred for few but large allocations.
====

When the JVM bootstrap, it requests a main memory of a certain size with the `PROT_NONE`
flag to prevent any access. This has the effect to tell the OS that this mapping should
not be backed by physical memory. Then when memory is needed by the program,
the JVM changes the mapping for a sub-range of that main memory by removing the
`PROT_NONE` flag. When new java threads are created, then the JVM will simply
request another memory segment.


.Simple C code example
[%collapsible]
====

To help you understand here's a very simple program:

. that *reserves* `16 MiB` via a `malloc` call and `16 MiB` via the `mmap` call
. then this program will invoke `ps` to show its actual memory consumption (RSS)
. then it will touch/use memory by setting a bit every `1 KiB`
. then this program will invoke `ps` again to show its actual memory consumption (RSS)

.memory example
[source,c,role="primary"]
----
#include <stdio.h>
#include <stdlib.h>
#include <unistd.h>
#include <sys/mman.h>

#define HEAP_SIZE (16 * 1024 * 1024 * sizeof(char))

int main (int argc, char *argv[])
{
  char *heap1 = malloc(HEAP_SIZE);
  char *heap2 = mmap(0,
                     HEAP_SIZE,
                     PROT_NONE | PROT_WRITE,
                     MAP_PRIVATE | MAP_NORESERVE | MAP_ANONYMOUS,
                     -1,
                     0);

  pid_t pid = getpid();
  printf("pid: %d\n", pid);

  char buffer[50];

  sprintf(buffer, "ps -p %d -o rss,vsz,command", pid);
  printf("Executing: '%s'\n", buffer);
  system(buffer);

  printf("Writing to some pages, but not all\n");

  for (char* i = heap1; i < (heap1 + HEAP_SIZE / 16); i += 1024) {
    *i = 0x01;
  }
  for (char* i = heap2; i < (heap2 + HEAP_SIZE / 8); i += 1024) {
    *i = 0x01;
  }

  sprintf(buffer, "ps -p %d -o rss,vsz,command", pid);
  printf("Executing: '%s'\n", buffer);
  system(buffer);

  free(heap1);
  munmap(heap2, HEAP_SIZE);

  return 0;
}
----

.result (Linux / llvm)
[source,shell,role="secondary"]
----
$ clang -Wall -Wpedantic -o test-alloc test-alloc.c && ./test-alloc
pid: 4301956

Executing: 'ps -p 2904 -o rss,vsz,command'
   RSS      VSZ COMMAND
   708  4301956 ./test-mem
Writing to some pages, but not all
Executing: 'ps -p 2904 -o rss,vsz,command'
   RSS      VSZ COMMAND
  3780  4301956 ./test-mem
----

As the `stdout` shows the RSS of this program is very low until memory
is actually written to. At the same time the virtual memory is much,
much higher; it means this simple program could address up to
about `4 GiB`.

_This program ran on a MacBook Pro 2018 running an Intel Core i7 CPU._
====



Now after some memory management refresh, let's go back to the main topic of this blog post.






==== Exploring what NMT does not show

The previous section showed that NMT numbers only represents the sizes
of the different JVM memory zones, but, does not reflect the real usage.


The JVM components reported by NMT can use different _types of memory management_ and
as such may have multiple allocation mechanisms. For example:

. GC based
The `Java heap` and the `Metaspace` (`Class`) are usually the biggest consumers of memory,
they both rely on `mmap`.
+
.java heap and metaspace
----
-                 Java Heap (reserved=3145728KB, committed=3145728KB)
                            (mmap: reserved=3145728KB, committed=3145728KB)

-                     Class (reserved=1195111KB, committed=164967KB)
                            (classes #27354)
                            (  instance classes #25689, array classes #1665)
                            (malloc=5223KB #86596)
                            (mmap: reserved=1189888KB, committed=159744KB)
----
+
These two _memory zones_ are interesting in that they are managed by the GC algorithm,
put in other words the GC is actually the memory manager of these zones, it is able to
_arrange_ the memory according to the options that are passed on the command line.
E.g. with a fixed size heap (`Xms` = `Xmx`), the heap will be constituted of a large memory
segment, in this case the _reserved_ and _committed_ values will be the same as well.
+
Other options may trigger specific behavior for these memory zones, e.g. make
the heap to grow or to shrink (I never saw that in practice,
maybe I'll see it once I use a JDK 12+ with _heap uncommit_ with https://openjdk.java.net/jeps/346[JEP-346],
although even the JEP mention it'll only happen if there is very low activity, which is unlikely to
happen for some workload).



. Threads
The Java threads are constructs controlled by the JVM runtime,
each thread is allocated on addressable space, their allocation size is always the
same, but can be controlled via a few JVM parameters. Their usage depends on
application usage. Eg. if the program request 1000 threads, then the JVM needs
to allocate 1000 threads.
+
.thread
----
-                    Thread (reserved=533903KB, committed=70439KB)
                            (thread #517)
                            (stack: reserved=531432KB, committed=67968KB) <1>
                            (malloc=1866KB #3103) <2>
                            (arena=605KB #1033) <3>
----
<1> The stack memory is where the JVM puts the thread stack, it's the sum
of all thread stack memory mappings.
<2> The thread sub-system performed 3103 ``malloc`` calls amounting to `1866 KiB`.
<3> The thread local handles required 1033 arenas, amounting to `605 KiB`.


. Other native zones
The other component reported by NMT management uses different technics. Sometime using a
combination of these technics:
+
`GC` zone for example only works with `malloc` and `mmap`, and size can grow as needed.
+
.gc
----
-                        GC (reserved=180505KB, committed=180505KB)
                            (malloc=30589KB #219593) <1>
                            (mmap: reserved=149916KB, committed=149916KB) <2>
----
<1> Here the GC performed 219593 ``malloc`` calls amounting to `30589 KiB`.
<2> Here the GC reserved and committed memory segment(s) amount to `149916 KiB`.
+
The JVM also implements its own
https://en.wikipedia.org/wiki/Region-based_memory_management[Arena based memory management],
(distinct from the arena memory management of glibc). It is used by some
subsystems of the JVM or when native code uses internal objects that rely on JVM arenas
https://github.com/corretto/corretto-11/blob/885a3859f47627467a15adaef36fd90ceb517f5e/src/src/hotspot/share/utilities/bitMap.hpp#L344-L345[[1\]]
https://github.com/corretto/corretto-11/blob/7ea9366e39d0650274e45ce966b36bb01d26ff26/src/src/hotspot/share/utilities/growableArray.hpp#L127[[2\]]
+
`Compiler`, `Symbol table` do use this memory management for example.
Special mention of the _thread local handles_ that also use JVM arenas.
+
NMT reports all the memory allocation technics that are used by a JVM component,
for example the GC system :
+
.compiler
----
-                  Compiler (reserved=6666KB, committed=6666KB)
                            (malloc=6533KB #3575) <1>
                            (arena=133KB #5) <2>
----
<1> The compiler performed 3575 ``malloc`` calls amounting to `6533 KiB`.
<2> The compiler uses 5 arenas totaling `133 KiB`.


There are three kinds of segments we can easily guess in the memory
mapping because we know their size, Java heap, metaspace, and threads.
Other segments are difficult to guess for two reasons: the malloc
implementation details, like the arenas in Glibc, and the number of
different `malloc` calls for a single component.




// TODO: introduce pmap better
==== heu

Where to look for this number? While it's easy to get the RSS of a process, to understand
if the committed heap actually _resides_ on physical memory you need to use `pmap` or
inspect `/proc/{pid}/maps` or `/proc/{pid}/smaps`. You have to notice the one of the first
memory zones is quite big and about the size of the committed heap as shown in NMT.
It's easier to spot with `pmap -X` (capital `X`).

.`pmap -x <pid>`
[source, role="primary"]
----
$ pmap -x $(pidof java) | less -S -X
6:   /usr/bin/java -Dfile.encoding=UTF-8 -Duser.timezone=UTC -Djava.security.egd=file:/dev/./urandom
Address           Kbytes     RSS   Dirty Mode  Mapping
0000000000400000       4       4       0 r-x-- java
0000000000600000       4       4       4 r---- java
0000000000601000       4       4       4 rw--- java
0000000001cfc000     412     224     224 rw---   [ anon ]
00000006f0000000 4477472 2944744 2944744 rw---   [ anon ] <1>
0000000801488000 1027552       0       0 -----   [ anon ]
00007f11b3744000   16388   16388   16388 rw---   [ anon ]
00007f11b4745000      16       0       0 -----   [ anon ]
00007f11b4749000   50688   49484   49484 rw---   [ anon ]
00007f11b78c9000    1536       0       0 -----   [ anon ]
00007f11b7a49000   32776   32776   32776 rw---   [ anon ]
00007f11b9a4b000      16       0       0 -----   [ anon ] <2>
00007f11b9a4f000    1012      24      24 rw---   [ anon ] <3>
00007f11b9b4c000      16       0       0 -----   [ anon ]
00007f11b9b50000    1012      92      92 rw---   [ anon ]
00007f11b9c4d000      16       0       0 -----   [ anon ]
00007f11b9c51000    1012     116     116 rw---   [ anon ]
...
----
<1> heap memory
<2> a thread guard pages
<3> a thread stack

.`pmap- X <pid>`
[source, role="secondary"]
----
$ pmap -X $(pidof java) | less -S -X
6:   /usr/bin/java -Dfile.encoding=UTF-8 -Duser.timezone=UTC -Djava.security.egd=file:/dev/./urandom -XX:InitialRAMPercentage=85.0 -XX:MaxRAMPercentage=85.0 -XX:NativeMemoryTracking=summary
         Address Perm   Offset Device   Inode     Size     Rss     Pss Referenced Anonymous LazyFree ShmemPmdMapped Shared_Hugetlb Private_Hugetlb Swap SwapPss Locked THPeligible Mapping
        00400000 r-xp 00000000  08:01 4054960        4       4       1          4         0        0              0              0               0    0       0      0           0 java
        00600000 r--p 00000000  08:01 4054960        4       4       4          4         4        0              0              0               0    0       0      0           0 java
        00601000 rw-p 00001000  08:01 4054960        4       4       4          4         4        0              0              0               0    0       0      0           0 java
        01cfc000 rw-p 00000000  00:00       0      412     224     224        224       224        0              0              0               0    0       0      0           0 [heap] <1>
       6f0000000 rw-p 00000000  00:00       0  4477472 2939592 2939592    2939592   2939592        0              0              0               0    0       0      0           0        <2>
       801488000 ---p 00000000  00:00       0  1027552       0       0          0         0        0              0              0               0    0       0      0           0
    7f11b4745000 ---p 00000000  00:00       0       16       0       0          0         0        0              0              0               0    0       0      0           0
    7f11b4749000 rw-p 00000000  00:00       0    50688   49472   49472      49472     49472        0              0              0               0    0       0      0           0
    7f11b78c9000 ---p 00000000  00:00       0     1536       0       0          0         0        0              0              0               0    0       0      0           0
    7f11b7a49000 rw-p 00000000  00:00       0    32776   32776   32776      32776     32776        0              0              0               0    0       0      0           0
    7f11b9a4b000 ---p 00000000  00:00       0       16       0       0          0         0        0              0              0               0    0       0      0           0        <3>
    7f11b9a4f000 rw-p 00000000  00:00       0     1012     112     112        112       112        0              0              0               0    0       0      0           0        <4>
    7f11b9b4c000 ---p 00000000  00:00       0       16       0       0          0         0        0              0              0               0    0       0      0           0
    7f11b9b50000 rw-p 00000000  00:00       0     1012      96      96         96        96        0              0              0               0    0       0      0           0
    7f11b9c4d000 ---p 00000000  00:00       0       16       0       0          0         0        0              0              0               0    0       0      0           0
    7f11b9c51000 rw-p 00000000  00:00       0     1012     116     116        116       116        0              0              0               0    0       0      0           0
...
----
<1> native heap memory
<2> java heap
<3> a thread guard pages
<4> a thread stack








==== Focusing on mapped files


The `NativeMemoryTracking` output showed memory usage of the JVM, but it didn't report
`MappedByteBuffers`, those are the files that are _memory mapped_ to the virtual memory
of a process as explained above via the native `mmap` call.
Memory pages of the file content that have been placed in RAM by the OS are accounted in RSS.



First let's see the memory mappings of a process using the handy command : `pmap -x <pid>`.
`pmap` is part of the https://gitlab.com/procps-ng/procps/[`procps`] utilities, that contains
other tools like: `ps`, `pgrep`, `watch` or `vmstat`. It's likely that no additional
installation is required which is great as a container filesystem should be read-only
for security reasons.

For example on the same process on which I showed the native memory.

.process memory mappings
[source, shell]
----
$ pmap -x $(pgrep java)
6:   /usr/bin/java -Dfile.encoding=UTF-8 -Duser.timezone=UTC -Djava.security.egd=file:/dev/./urandom
-XX:InitialRAMPercentage=85.0 -XX:MaxRAMPercentage=85.0 -XX:NativeMemoryTracking=summary
-Xlog:os,safepoint*,gc*,gc+ref=debug,gc+ergo*=debug,gc+age*=debug,gc+phases*:file=/gclogs/%t-gc.log:time,uptime,tags:filecount=5,filesize=10M -javaag
Address           Kbytes     RSS   Dirty Mode  Mapping
0000000000400000       4       4       0 r-x-- java
0000000000600000       4       4       4 r---- java
0000000000601000       4       4       4 rw--- java
000000000216f000     404     272     272 rw---   [ anon ]
00000006f0000000 4476620 3128252 3128252 rw---   [ anon ]
00000008013b3000 1028404       0       0 -----   [ anon ]
00007fc5de9ea000      16       0       0 -----   [ anon ]
00007fc5de9ee000    1012     104     104 rw---   [ anon ]
00007fc5deaeb000      16       0       0 -----   [ anon ]
00007fc5deaef000    1012      24      24 rw---   [ anon ]
00007fc5debec000      16       0       0 -----   [ anon ]
00007fc5debf0000    1012      92      92 rw---   [ anon ]
00007fc5deced000      16       0       0 -----   [ anon ]
00007fc5decf1000    1012     100     100 rw---   [ anon ]
00007fc5dedee000      16       0       0 -----   [ anon ]
00007fc5dedf2000    1012     100     100 rw---   [ anon ]
00007fc5deeef000      16       0       0 -----   [ anon ]
00007fc5deef3000    1012     100     100 rw---   [ anon ]
00007fc5deff0000      16       0       0 -----   [ anon ]
00007fc5deff4000    1012     100     100 rw---   [ anon ]
00007fc5df0f1000      16       0       0 -----   [ anon ]
00007fc5df0f5000    1012     100     100 rw---   [ anon ]
00007fc5df1f2000      16       0       0 -----   [ anon ]
00007fc5df1f6000    1012     100     100 rw---   [ anon ]
00007fc5df2f3000      16       0       0 -----   [ anon ]
00007fc5df2f7000    1012     100     100 rw---   [ anon ]
00007fc5df3f4000      16       0       0 -----   [ anon ]
00007fc5df3f8000    1012     100     100 rw---   [ anon ]
00007fc5df4f5000      16       0       0 -----   [ anon ]
00007fc5df4f9000    1012     100     100 rw---   [ anon ]
00007fc5df5f6000      16       0       0 -----   [ anon ]
00007fc5df5fa000    1012     100     100 rw---   [ anon ]

...

00007fca48ba9000   17696   14876       0 r-x-- libjvm.so
00007fca49cf1000    2044       0       0 ----- libjvm.so
00007fca49ef0000     764     764     764 r---- libjvm.so
00007fca49faf000     232     232     208 rw--- libjvm.so
00007fca49fe9000     352     320     320 rw---   [ anon ]
00007fca4a041000     136     136       0 r---- libc-2.28.so
00007fca4a063000    1312    1140       0 r-x-- libc-2.28.so
00007fca4a1ab000     304     148       0 r---- libc-2.28.so
00007fca4a1f7000       4       0       0 ----- libc-2.28.so
00007fca4a1f8000      16      16      16 r---- libc-2.28.so
00007fca4a1fc000       8       8       8 rw--- libc-2.28.so
00007fca4a1fe000      16      16      16 rw---   [ anon ]
00007fca4a202000       4       4       0 r---- libdl-2.28.so
00007fca4a203000       4       4       0 r-x-- libdl-2.28.so
00007fca4a204000       4       4       0 r---- libdl-2.28.so
00007fca4a205000       4       4       4 r---- libdl-2.28.so
00007fca4a206000       4       4       4 rw--- libdl-2.28.so
00007fca4a207000     100     100       0 r-x-- libjli.so
00007fca4a220000    2048       0       0 ----- libjli.so
00007fca4a420000       4       4       4 r---- libjli.so
00007fca4a421000       4       4       4 rw--- libjli.so
00007fca4a422000      24      24       0 r---- libpthread-2.28.so
00007fca4a428000      60      60       0 r-x-- libpthread-2.28.so
00007fca4a437000      24       0       0 r---- libpthread-2.28.so
00007fca4a43d000       4       4       4 r---- libpthread-2.28.so
00007fca4a43e000       4       4       4 rw--- libpthread-2.28.so
00007fca4a43f000      16       4       4 rw---   [ anon ]
00007fca4a443000       4       4       0 r---- LC_IDENTIFICATION
00007fca4a444000       4       0       0 -----   [ anon ]
00007fca4a445000       4       0       0 r----   [ anon ]
00007fca4a446000       8       8       8 rw---   [ anon ]
00007fca4a448000       4       4       0 r---- ld-2.28.so
00007fca4a449000     120     120       0 r-x-- ld-2.28.so
00007fca4a467000      32      32       0 r---- ld-2.28.so
00007fca4a46f000       4       4       4 r---- ld-2.28.so
00007fca4a470000       4       4       4 rw--- ld-2.28.so
00007fca4a471000       4       4       4 rw---   [ anon ]
00007ffe28536000     140      40      40 rw---   [ stack ]
00007ffe28582000      12       0       0 r----   [ anon ]
00007ffe28585000       8       4       0 r-x--   [ anon ]
ffffffffff600000       4       0       0 r-x--   [ anon ]
---------------- ------- ------- -------
total kB         24035820 4776860 4720796
----

To select the file mappings we can filter on the
https://www.kernel.org/doc/Documentation/filesystems/proc.txt[access permissions]:

* `r-`: readable memory mapping
* `w`: writable memory mapping
* `x`: executable memory mapping
* `s` or `p` : shared memory mapping or private mapping. `/proc/<pid>/maps`

[INFO]
=======
On a side note, `pmap` may show another mapping mode which I barely found any
reference of, here's https://johanlouwers.blogspot.com/2017/07/oracle-linux-understanding-linux.html[one]
and https://linux.die.net/man/2/mmap[here]

* `R`: if set, the map has no swap space reserved (`MAP_NORESERVE` flag of `mmap`).
This means that we can get a segmentation fault by accessing that memory if it has not
already been mapped to physical memory, and if the system is out of physical memory.
=======

At this time the focus is to see what are the memory mapped files with the JVM.
The `Mapping` column on the of `pmap -x $(pgrep java)` can be parsed to identify
file mappings, but this is brittle and unnecessary, one can simply look at
the output of `pmap -X $(pgrep java)` (notice the big `X`) or even at the
`/proc/$(pidof java)/maps` content looking for a non-zero value of the `inode`
column indicating this is a file.

Using the output of `pmap -X $(pgrep java)` and selecting the matching lines
with `awk` this is _easy_:

.Shared application memory mapped files
[source, shell]
----
$ pmap -X $(pidof java) | awk '{ if (NR <= 2 || $5 >0 ) \ <1>
  printf "%12s %8s %8s %4s %s\n", \ <2>
  $1, \
  $6, \
  $7, \
  $2, \
  $19 }' <2>
          7: -Djava.awt.headless=true -XX:NativeMemoryTracking=summary /usr/bin/java
     Address     Size      Rss Perm Mapping <3>
561ddb94a000        4        4 r-xp java
561ddbb4b000        4        4 r--p java
561ddbb4c000        4        4 rw-p java
7f355521f000        4        4 r--s instrumentation9549273990865322165.jar
7f355964d000        4        4 r--s instrumentation14393425676176063484.jar
7f3559e50000     1160     1160 r--s dd-java-agent.jar
7f355a372000      256      192 r-xp libsunec.so
7f355a3b2000     2048        0 ---p libsunec.so
7f355a5b2000       20       20 r--p libsunec.so
7f355a5b7000        8        8 rw-p libsunec.so
7f355a7b9000       16       16 r--p libresolv-2.28.so
7f355a7bd000       52       52 r-xp libresolv-2.28.so
7f355a7ca000       16       16 r--p libresolv-2.28.so
7f355a7ce000        4        0 ---p libresolv-2.28.so
7f355a7cf000        4        4 r--p libresolv-2.28.so
7f355a7d0000        4        4 rw-p libresolv-2.28.so
7f355a7d3000        4        4 r--p libnss_dns-2.28.so
7f355a7d4000       16       16 r-xp libnss_dns-2.28.so
7f355a7d8000        4        0 r--p libnss_dns-2.28.so
7f355a7d9000        4        4 r--p libnss_dns-2.28.so
7f355a7da000        4        4 rw-p libnss_dns-2.28.so
7f355a7dd000        4        4 r--s instrumentation13129117816180832587.jar
7f355a7de000        8        8 r-xp libextnet.so
7f355a7e0000     2044        0 ---p libextnet.so
7f355a9df000        4        4 r--p libextnet.so
7f355b9e9000        4        4 r--s newrelic-bootstrap1151474907525430822.jar
7f355bfea000       24       24 r-xp libmanagement_ext.so
7f355bff0000     2044        0 ---p libmanagement_ext.so
7f355c1ef000        4        4 r--p libmanagement_ext.so
7f355c1f0000        4        4 rw-p libmanagement_ext.so
7f355c1f1000       16       16 r-xp libmanagement.so
7f355c1f5000     2048        0 ---p libmanagement.so
7f355c3f5000        4        4 r--p libmanagement.so
7f355c5f7000        8        8 r--s newrelic-weaver-api14962018995408739070.jar
7f355c5f9000       12       12 r--s newrelic-api8237374132620194936.jar
7f355c5fc000        4        4 r--s newrelic-opentracing-bridge6621669571490510163.jar
7f355c5fd000       16       16 r--s agent-bridge7978421659510986627.jar
7f355c601000       88       88 r-xp libnet.so
7f355c617000     2048        0 ---p libnet.so
7f355c817000        4        4 r--p libnet.so
7f355c818000        4        4 rw-p libnet.so
7f355c819000       64       64 r-xp libnio.so
7f355c829000     2048        0 ---p libnio.so
7f355ca29000        4        4 r--p libnio.so
7f355ca2a000        4        4 rw-p libnio.so
7f355cf30000      200      128 r--p LC_CTYPE
7f355cf62000        4        4 r--p LC_NUMERIC
7f355cf63000        4        4 r--p LC_TIME
7f355cf64000     1484      156 r--p LC_COLLATE
7f355d0d7000        4        4 r--p LC_MONETARY
7f355d0d8000        4        4 r--p SYS_LC_MESSAGES
7f355d0d9000        4        4 r--p LC_PAPER
7f355d0da000        4        4 r--p LC_NAME
7f355d0db000       28       28 r--s gconv-modules.cache
7f357663b000   138232    30036 r--s modules
7f357ed39000      104       92 r-xp libzip.so
7f357ed53000     2044        0 ---p libzip.so
7f357ef52000        4        4 r--p libzip.so
7f357ef5c000       12       12 r--p libnss_files-2.28.so
7f357ef5f000       28       28 r-xp libnss_files-2.28.so
7f357ef66000        8        8 r--p libnss_files-2.28.so
7f357ef68000        4        0 ---p libnss_files-2.28.so
7f357ef69000        4        4 r--p libnss_files-2.28.so
7f357ef6a000        4        4 rw-p libnss_files-2.28.so
7f357ef71000        4        4 r--p LC_ADDRESS
7f357ef72000        4        4 r--p LC_TELEPHONE
7f357ef73000        4        4 r--p LC_MEASUREMENT
7f357ef74000       40       40 r-xp libinstrument.so
7f357ef7e000     2044        0 ---p libinstrument.so
7f357f17d000        4        4 r--p libinstrument.so
7f357f17e000        4        4 rw-p libinstrument.so
7f357f17f000      108       64 r-xp libjimage.so
7f357f19a000     2048        0 ---p libjimage.so
7f357f39a000        8        8 r--p libjimage.so
7f357f39c000        4        4 rw-p libjimage.so
7f357f39d000      164      164 r-xp libjava.so
7f357f3c6000     2048        0 ---p libjava.so
7f357f5c6000        4        4 r--p libjava.so
7f357f5c7000        4        4 rw-p libjava.so
7f357f5c9000       68       68 r-xp libverify.so
7f357f5da000     2044        0 ---p libverify.so
7f357f7d9000        8        8 r--p libverify.so
7f357f7dc000        8        8 r--p librt-2.28.so
7f357f7de000       16       16 r-xp librt-2.28.so
7f357f7e2000        8        0 r--p librt-2.28.so
7f357f7e4000        4        4 r--p librt-2.28.so
7f357f7e5000        4        4 rw-p librt-2.28.so
7f357f8e7000    17680    15012 r-xp libjvm.so
7f3580a2b000     2044        0 ---p libjvm.so
7f3580c2a000      764      764 r--p libjvm.so
7f3580ce9000      228      228 rw-p libjvm.so
7f3580d7d000       12       12 r--p libgcc_s.so.1
7f3580d80000       68       64 r-xp libgcc_s.so.1
7f3580d91000       12       12 r--p libgcc_s.so.1
7f3580d94000        4        0 ---p libgcc_s.so.1
7f3580d95000        4        4 r--p libgcc_s.so.1
7f3580d96000        4        4 rw-p libgcc_s.so.1
7f3580d97000       52       52 r--p libm-2.28.so
7f3580da4000      636      368 r-xp libm-2.28.so
7f3580e43000      852      128 r--p libm-2.28.so
7f3580f18000        4        4 r--p libm-2.28.so
7f3580f19000        4        4 rw-p libm-2.28.so
7f3580f1a000      548      548 r--p libstdc++.so.6.0.25
7f3580fa3000      688      192 r-xp libstdc++.so.6.0.25
7f358104f000      248       64 r--p libstdc++.so.6.0.25
7f358108d000        4        0 ---p libstdc++.so.6.0.25
7f358108e000       40       40 r--p libstdc++.so.6.0.25
7f3581098000        8        8 rw-p libstdc++.so.6.0.25
7f35810a0000      136      136 r--p libc-2.28.so
7f35810c2000     1312     1208 r-xp libc-2.28.so
7f358120a000      304      152 r--p libc-2.28.so
7f3581256000        4        0 ---p libc-2.28.so
7f3581257000       16       16 r--p libc-2.28.so
7f358125b000        8        8 rw-p libc-2.28.so
7f3581261000        4        4 r--p libdl-2.28.so
7f3581262000        4        4 r-xp libdl-2.28.so
7f3581263000        4        4 r--p libdl-2.28.so
7f3581264000        4        4 r--p libdl-2.28.so
7f3581265000        4        4 rw-p libdl-2.28.so
7f3581266000      100      100 r-xp libjli.so
7f358127f000     2048        0 ---p libjli.so
7f358147f000        4        4 r--p libjli.so
7f3581480000        4        4 rw-p libjli.so
7f3581481000       24       24 r--p libpthread-2.28.so
7f3581487000       60       60 r-xp libpthread-2.28.so
7f3581496000       24        0 r--p libpthread-2.28.so
7f358149c000        4        4 r--p libpthread-2.28.so
7f358149d000        4        4 rw-p libpthread-2.28.so
7f35814a2000        4        4 r--p LC_IDENTIFICATION
7f3581878000        4        4 r--p ld-2.28.so
7f3581879000      120      120 r-xp ld-2.28.so
7f3581897000       32       32 r--p ld-2.28.so
7f358189f000        4        4 r--p ld-2.28.so
7f35818a0000        4        4 rw-p ld-2.28.so
     ======= ======== ============== =======
     6172856        0        0 4245724 <4>
----
<1> Filter lines that have an Inode value over 0 and only from the 3rd line (included).
<2> Print only some columns, `pmap -X {pid}`'s output is verbose.
<3> The columns are select to match the output of `pmap -x`, `Size` column is in `KiB`.
<4> The last two lines aren't filtered out due to my limited skills in `awk` ; the actual
sums of the _size_ and _rss_ columns of the selected rows are respectively
`195336 KiB` and `52316 KiB`.

What may catch the eye is the multiple mapping for native libraries like `libjvm.so`.
The reason is that some part of the file is executable, others are for exchanging information
with the library.

* `r-xp` means an executable segment of the library
* `r\--p` means readable memory of the library, e.g. constants
* `rw-p` means writable memory, this is usually where the process can set global
variables of thw library.
*  `---p` is a no permission segment, I'm not sure about this one, but it's location
(between executable and writable segments) makes me think it's about buffer overflow
prevention.

Mapped files represents `195.3 MiB` of the address space of which `52.3 MiB` are
actually resident.










Wrapping this information from NMT and memory mapped files leaves us with the
following _equation_ to estimate the actual memory usage of a process:

....
Total memory = Heap + GC + Metaspace + Code Cache + Symbol tables
               + Compiler + Other JVM structures + Thread stacks
               + Direct buffers + Mapped files +
               + Native Libraries + Malloc overhead + ...
....

Using the *committed* values from the NMT output above, and the *mapping size*
this breaks out as :

[%autowidth.stretch,options="footer"]
|===

| Heap                            | 4456448
| GC                              |  230739
| Metaspace                       |  165788
| Code Cache                      |  105201
| Symbol tables                   |   28915
| Compiler                        |    5914
| Other JVM structures
(Internal + NMT + smaller area)   |   24460 + 8433 + 217 + 7 + 19 + 1362 + 837 + 8 + 32 = (35375)
| Thread stacks                   |   85455
| Direct buffers (Other)          |  267034

| Total accounted by NMT          | 5380868

| Mapped files (and native libs)  |  195336
| Malloc overhead                 | accounted in NMT
| ...                             |

| Total                           | 5576205 KiB
|===


`5576205 KiB` is what this container is supposedly actually using, but:

 - as expected this is way over the RSS (`4701120 KiB`) and,
 - also over the `5 GiB` (`5242880 KiB`) of the pod limit.

This happens because this pod may not have access all pages especially
if the heap is big enough and the allocation rate is not big enough. While
a pod like this may be healthy and stay that way. It should rise your eyebrow,
if the RSS grow then the pod is likely to be oomkilled.

This is where one need to review the memory parameter of either
the JVM or the memory limits of the pod. Either the JVM is over-sized
or the JVM will eventually be oomkilled by the OS.






===== How many pages are used ?

The _proc_ filesystem is gives the paging details on the current process.

----
Table 1-3: Contents of the statm files (as of 2.6.8-rc3)
..............................................................................
 Field    Content
 size     total program size (pages)		(same as VmSize in status)
 resident size of memory portions (pages)	(same as VmRSS in status)
 shared   number of pages that are shared	(i.e. backed by a file, same
						as RssFile+RssShmem in status)
 trs      number of pages that are 'code'	(not including libs; broken,
							includes data segment)
 lrs      number of pages of library		(always 0 on 2.6)
 drs      number of pages of data/stack		(including libs; broken,
							includes library text)
 dt       number of dirty pages			(always 0 on 2.6)

----

[source, shell]
----
$ ps -o rss,vsz,command $(pidof java)
  RSS    VSZ COMMAND
4346704 6507368 /usr/bin/java -Dfile.encoding=UTF-8 -Duser.timezone=UTC -Djava.security.egd=file:/dev/./urandom -Djava
$ cat /proc/$(pidof java)/statm | tr ' ' '\n'
1626842 <1>
1086676 <2>
12638
1
0
1283103
0
----
<1> Total size in _pages_ of the addressing space, in bytes : `6507368 KiB`
<2> Resident memory in _pages_, in bytes : `4346704 KiB`

Given a page size is `4 KiB`, it gives in particular :

* vsz = `1626842 * 4 = 6507368`
* rss = `1086676 * 4 = 4346704`

NOTE: If the kubernetes memory limit is `5 GiB` (`5242880 KiB`),
then the process may be oom-killed if the program uses more than
`1310720` used pages. If the program don't use more pages than this
number, this process will be fine.



////
[TIP]
=====
While NMT show you the total, you can use this command to extract all relevant from the summary
in a simple addition

[source,bash]
----
echo $(($(jcmd $(pidof java) VM.native_memory \
  | tee /dev/tty \
  | grep -P "^-.*committed=" \
  | grep -o -P "(?<=committed=)[0-9]+(?=KB)" \
  | awk 'BEGIN { ORS=""; print "(" }; {print p$0; p=" + "} END { print ")\n"}' \
  | tee /dev/tty )))
----

On macOS, you should install the GNU coreutils and use `ggrep` and `ghead`.
=====
////




// TODO: move somewhere else ?
=== Virtual memory and paging effect on the Java heap

*Virtual memory* is a memory management scheme that is used by most operating systems ;
it allows programs to use memory without dealing with hardware, or other concerns like
sharing the memory resource. In doing so it allows programs to request more memory than
available. In this scheme the OS splits the virtual memory and the memory in smaller chunks
called *pages*. For any given page in the virtual memory, and depending on the application(s)
the OS may:

* Make this page resident in physical memory, if something has be written into it.
* Do nothing if a page is not used, this page is virtually available.
* Move a page from physical memory to swap, if the OS thinks there's not enough room for other pages.
* Map a portion of a file to this page.

image:../../static/assets/maxrampercentage/os-memory-paging.svg[align="center", title="Simple overview of OS paging"]

E.g at the moment this report was executed the committed memory is `5380868 KiB` (`5.13 GiB`) while
the process RSS is `4701120 KiB`. The difference relates to how `mmap` works (on Linux), memory
pages are only backed by physical memory once they're written to.

Some people may have heard of the `-XX:+AlwaysPreTouch` Hotspot option. This option tells
the JVM to https://github.com/corretto/corretto-11/blob/3b31d243a19774bebde63df21cc84e994a89439a/src/src/hotspot/share/runtime/os.cpp#L1825-L1829[write a zero to every OS memory pages].
This option has the effect of avoiding physical memory commit latencies at runtime, however this
only affects the heap memory zone. Other areas like thread stack or metaspace work differently.

In other words that means parts of the *committed* memory shown in NMT is not *resident* and as such
RSS counter may not reflect what is een in the *committed* memory.

